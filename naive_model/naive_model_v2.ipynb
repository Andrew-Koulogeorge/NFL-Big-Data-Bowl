{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Model (Play-Level Data) v2\n",
    "\n",
    "__Date:__ 11/5/2023 <br>\n",
    "__Purpose:__ Program that uses the play-level data to predict expected yards gained <br>\n",
    "__Model and data specifications:__\n",
    "- Data: Plays dataframe and some stuff from games df (no outside supplemental data)\n",
    "- Models: Basic supervised learning \n",
    "<br>__Updates from previous version:__ More efficient design of hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, roc_auc_score, auc, f1_score, accuracy_score, roc_curve, RocCurveDisplay, r2_score\n",
    "import time \n",
    "import sys\n",
    "sys.path.append('../preprocessing')\n",
    "from Preprocessing_v3 import *\n",
    "from DataLoader import load_data\n",
    "\n",
    "# Regression models\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded games df\n",
      "shape: (136, 9)\n",
      "-----\n",
      "loaded players df\n",
      "shape: (1683, 7)\n",
      "-----\n",
      "loaded plays df\n",
      "shape: (12486, 35)\n",
      "-----\n",
      "loading tracking frames...\n",
      "loaded tracking frames\n",
      "shape: (12187398, 17)\n",
      "returning 4 frames\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "[games_df, players_df, plays_df, tracking_df] = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that finishes preprocessing and does the train test split of plays df\n",
    "def plays_train_test_split(plays_df_clean):\n",
    "    # Drop game and play ID\n",
    "    plays_df_clean = plays_df_clean.drop(['gameId', 'playId'], axis = 1)\n",
    "    \n",
    "    # Get X and y matrices\n",
    "    y = plays_df_clean[\"TARGET\"]\n",
    "    X = plays_df_clean.drop([\"TARGET\"], axis = 1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=24)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that does cross validation and gives best model\n",
    "def run_cv(model, param_grid, X_train, y_train, X_val):\n",
    "    print(\"training \" + str(model))\n",
    "\n",
    "    # Define the cross-validation strategy\n",
    "    cv = KFold(n_splits=5)\n",
    "\n",
    "    # Get the type of scoring for the grid search depending on regression or classification\n",
    "    if model in [LinearRegression, Lasso, Ridge, ElasticNet, SVR, RandomForestRegressor, AdaBoostRegressor, XGBRegressor]:\n",
    "        scoring_metric = 'neg_mean_squared_error'\n",
    "    else:\n",
    "        scoring_metric = 'f1_weighted'\n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    start_time = time.time()\n",
    "    grid_search = GridSearchCV(estimator=model(), param_grid=param_grid, cv=cv, scoring=scoring_metric)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    y_pred = grid_search.predict(X_val)\n",
    "\n",
    "    # Return the best model, y_pred\n",
    "    return grid_search, y_pred, train_time # return the metric and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of models and parameters\n",
    "regression_models = {LinearRegression : {},\n",
    "                     Lasso : {'alpha': [0.001, 0.01, 0.1, 1, 2]},\n",
    "                     Ridge :  {'alpha': [0.001, 0.01, 0.1, 1, 2]},\n",
    "                     ElasticNet : {'alpha': [0.001, 0.01, 0.1, 1, 2], \n",
    "                                   'l1_ratio': [0.1, 0.25, 0.5, 0.75, 0.9]},\n",
    "                     # SVR : {'C': [0.01, 0.1, 1, 2, 10], 'kernel': ['linear', 'poly', 'rbf']},\n",
    "                     RandomForestRegressor : {'n_estimators': [100, 500, 1000],\n",
    "                                              'max_depth': [100, None]},\n",
    "                     AdaBoostRegressor : {'n_estimators': [50, 100, 200],\n",
    "                                          'learning_rate': [0.001, 0.01, 0.1, 1, 2]},\n",
    "                     XGBRegressor : {'max_depth': [3, 5, 6, 7], \n",
    "                                     'learning_rate': [0.1, 0.3, 0.5], \n",
    "                                     'subsample': [0.5, 0.7, 1]}                    \n",
    "}\n",
    "\n",
    "classification_models = {\n",
    "    # SVR : {'C': [0.01, 0.1, 1, 2, 10], 'kernel': ['linear', 'poly', 'rbf']},\n",
    "    RandomForestClassifier : {'n_estimators': [100, 500, 1000],\n",
    "                            'max_depth': [100, None]},\n",
    "    AdaBoostClassifier : {'n_estimators': [50, 100, 200],\n",
    "                        'learning_rate': [0.001, 0.01, 0.1, 1, 2]},\n",
    "    XGBClassifier : {'max_depth': [3, 5, 6, 7], \n",
    "                    'learning_rate': [0.1, 0.3, 0.5], \n",
    "                    'subsample': [0.5, 0.7, 1]} ,\n",
    "    LogisticRegression : {'penalty': ['l1', 'l2', 'elasticnet', None]},\n",
    "    GaussianNB : {},\n",
    "    Perceptron : {'penalty': ['l1', 'l2', 'elasticnet']}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final plays data shape: (6840, 289)\n",
      "training <class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "training <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nt/y2pysqbx2wg4rrlhdnq0djrc0000gn/T/ipykernel_65501/3189500133.py:74: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  results_df = pd.concat([results_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training <class 'xgboost.sklearn.XGBClassifier'>\n"
     ]
    }
   ],
   "source": [
    "include_nfl_features_params = [True, False]\n",
    "bin_ouput_params = [True, False]\n",
    "\n",
    "results_df = pd.DataFrame(columns = ['model', 'regression/classification', 'train_time',\n",
    "                                                'MSE pre-bin', 'r2_score',\n",
    "                                                'bin_output', 'include_nfl_features',\n",
    "                                                'f1_score','confusion_matrix', 'accuracy_score'])\n",
    "start_time = time.time()\n",
    "for include_nfl_features in include_nfl_features_params:\n",
    "    for bin_output in bin_ouput_params: \n",
    "        # Prepreocessing \n",
    "        plays_df_clean = preprocess_plays_df_naive_models(plays_df, games_df, include_nfl_features, bin_output)\n",
    "\n",
    "        # Train test split\n",
    "        X_train, X_test, y_train, y_test = plays_train_test_split(plays_df_clean)\n",
    "\n",
    "        # Check if we need to do regression first\n",
    "        if not bin_output:\n",
    "            for model_class in regression_models.keys():\n",
    "                # Train model\n",
    "                model, y_pred, train_time = run_cv(model_class, regression_models[model_class], X_train, y_train, X_test)\n",
    "\n",
    "                # Get regression accuracy\n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "                # Bin both and get post-binned metrics\n",
    "                bins = [float('-inf'), -2, 0, 1, 2.5, 5, 10, float('inf')]\n",
    "                y_pred_binned = pd.cut(y_pred, bins = bins, labels = range(len(bins) - 1))\n",
    "                y_test_binned = pd.cut(y_test, bins = bins, labels = range(len(bins) - 1))\n",
    "\n",
    "                f1_metric = f1_score(y_test_binned, y_pred_binned, average = 'weighted')\n",
    "                confusion_mat = confusion_matrix(y_test_binned, y_pred_binned)\n",
    "                accuracy = accuracy_score(y_test_binned, y_pred_binned)\n",
    "\n",
    "                # Record result\n",
    "                new_row = pd.DataFrame({\n",
    "                    'model': [str(model.best_estimator_)],\n",
    "                    'regression/classification': ['classification'], \n",
    "                    'train_time': [train_time],\n",
    "                    'MSE pre-bin': [mse], \n",
    "                    'r2_score': [r2],\n",
    "                    'bin_output': [bin_output], \n",
    "                    'include_nfl_features': [include_nfl_features],\n",
    "                    'f1_score': [f1_metric],\n",
    "                    'confusion_matrix': [confusion_mat], \n",
    "                    'accuracy_score': [accuracy]\n",
    "                })\n",
    "                results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "        else:\n",
    "            for model_class in classification_models.keys():\n",
    "                # Train model\n",
    "                model, y_pred, train_time = run_cv(model_class, classification_models[model_class], X_train, y_train, X_test)\n",
    "\n",
    "                # Get accuracy metrics\n",
    "                f1_metric = f1_score(y_test, y_pred, average = 'weighted')\n",
    "                confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "                # Record result\n",
    "                new_row = pd.DataFrame({\n",
    "                    'model': str(model.best_estimator_),\n",
    "                    'regression/classification': ['classification'], \n",
    "                    'train_time': [train_time],\n",
    "                    'MSE pre-bin': [np.nan], \n",
    "                    'r2_score': [np.nan],\n",
    "                    'bin_output': [bin_output], \n",
    "                    'include_nfl_features': [include_nfl_features],\n",
    "                    'f1_score': [f1_metric],\n",
    "                    'confusion_matrix': [confusion_mat], \n",
    "                    'accuracy_score': [accuracy]\n",
    "                })\n",
    "                results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "print(\"total time: \" + str(time.time() - start_time))\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Get best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>regression/classification</th>\n",
       "      <th>train_time</th>\n",
       "      <th>MSE pre-bin</th>\n",
       "      <th>r2_score</th>\n",
       "      <th>bin_output</th>\n",
       "      <th>include_nfl_features</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>accuracy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaBoostClassifier(learning_rate=1, n_estimato...</td>\n",
       "      <td>classification</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.227863</td>\n",
       "      <td>[[6, 8, 1, 3, 61, 7, 4], [6, 16, 7, 12, 117, 2...</td>\n",
       "      <td>0.283041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RandomForestClassifier(max_depth=100)</td>\n",
       "      <td>classification</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.222277</td>\n",
       "      <td>[[3, 5, 10, 5, 53, 11, 3], [0, 20, 14, 15, 100...</td>\n",
       "      <td>0.272515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier()</td>\n",
       "      <td>classification</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.219749</td>\n",
       "      <td>[[2, 6, 9, 3, 55, 14, 1], [0, 16, 16, 15, 106,...</td>\n",
       "      <td>0.277193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>classification</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.218626</td>\n",
       "      <td>[[0, 6, 9, 9, 48, 16, 2], [3, 23, 19, 15, 95, ...</td>\n",
       "      <td>0.255556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>classification</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.218369</td>\n",
       "      <td>[[3, 8, 10, 4, 42, 18, 5], [3, 25, 16, 19, 83,...</td>\n",
       "      <td>0.246784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model  \\\n",
       "1   AdaBoostClassifier(learning_rate=1, n_estimato...   \n",
       "13              RandomForestClassifier(max_depth=100)   \n",
       "0                            RandomForestClassifier()   \n",
       "15  XGBClassifier(base_score=None, booster=None, c...   \n",
       "2   XGBClassifier(base_score=None, booster=None, c...   \n",
       "\n",
       "   regression/classification  train_time  MSE pre-bin  r2_score bin_output  \\\n",
       "1             classification    0.000055          NaN       NaN       True   \n",
       "13            classification    0.000090          NaN       NaN       True   \n",
       "0             classification    0.000088          NaN       NaN       True   \n",
       "15            classification    0.000027          NaN       NaN       True   \n",
       "2             classification    0.000036          NaN       NaN       True   \n",
       "\n",
       "   include_nfl_features  f1_score  \\\n",
       "1                  True  0.227863   \n",
       "13                False  0.222277   \n",
       "0                  True  0.219749   \n",
       "15                False  0.218626   \n",
       "2                  True  0.218369   \n",
       "\n",
       "                                     confusion_matrix  accuracy_score  \n",
       "1   [[6, 8, 1, 3, 61, 7, 4], [6, 16, 7, 12, 117, 2...        0.283041  \n",
       "13  [[3, 5, 10, 5, 53, 11, 3], [0, 20, 14, 15, 100...        0.272515  \n",
       "0   [[2, 6, 9, 3, 55, 14, 1], [0, 16, 16, 15, 106,...        0.277193  \n",
       "15  [[0, 6, 9, 9, 48, 16, 2], [3, 23, 19, 15, 95, ...        0.255556  \n",
       "2   [[3, 8, 10, 4, 42, 18, 5], [3, 25, 16, 19, 83,...        0.246784  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(by = 'f1_score', ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(by = 'f1_score', ascending = False).iloc[0]['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depreciated - run through on one dataset/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get X and y matrices\n",
    "# y = plays_df_clean[\"TARGET\"]\n",
    "# X = plays_df_clean.drop([\"TARGET\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=24)\n",
    "\n",
    "# print('X_train shape : ', X_train.shape)\n",
    "# print('y_train shape : ', y_train.shape)\n",
    "\n",
    "# print('X_test shape  : ', X_test.shape)\n",
    "# print('y_test shape  : ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LinearRegression()\n",
    "\n",
    "# model.__class__\n",
    "\n",
    "# # Get the type of scoring for the grid search depending on regression or classification\n",
    "# if model.__class__ in [LinearRegression, Lasso, Ridge, ElasticNet, SVR, RandomForestRegressor, AdaBoostRegressor, XGBRegressor]:\n",
    "#     scoring_metric = 'neg_mean_squared_error'\n",
    "# else:\n",
    "#     scoring_metric = 'f1_weighted'\n",
    "\n",
    "# print(scoring_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train model\n",
    "\n",
    "    \n",
    "\n",
    "# grid_search = GridSearchCV(estimator=LinearRegression(), param_grid={}, cv=KFold(5), scoring='neg_mean_squared_error')\n",
    "\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# y_pred = grid_search.predict(X_test)\n",
    "\n",
    "\n",
    "# # Get accuracy\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# # Bin both and get post-binned metrics\n",
    "# bins = [float('-inf'), -2, 0, 1, 2.5, 5, 10, float('inf')]\n",
    "# y_pred_binned = pd.cut(y_pred, bins = bins, labels = range(len(bins) - 1))\n",
    "# y_test_binned = pd.cut(y_test, bins = bins, labels = range(len(bins) - 1))\n",
    "\n",
    "# f1_metric = f1_score(y_test_binned, y_pred_binned, average = 'weighted')\n",
    "# confusion_mat = confusion_matrix(y_test_binned, y_pred_binned)\n",
    "# accuracy = accuracy_score(y_test_binned, y_pred_binned)\n",
    "\n",
    "# # Record result\n",
    "# new_row = pd.DataFrame({\n",
    "#     'model': [str(model.best_estimator_)],\n",
    "#     'regression/classification': ['classification'], \n",
    "#     'train_time': [train_time],\n",
    "#     'MSE pre-bin': [mse], \n",
    "#     'r2_score': [r2],\n",
    "#     'bin_output': [bin_output], \n",
    "#     'include_nfl_features': [include_nfl_features],\n",
    "#     'f1_score': [f1_metric],\n",
    "#     'confusion_matrix': [confusion_mat], \n",
    "#     'accuracy_score': [accuracy]\n",
    "# })\n",
    "# print(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepreocessing \n",
    "# plays_df_clean = preprocess_plays_df_naive_models(plays_df, games_df, True, False)\n",
    "\n",
    "# # Train test split\n",
    "# X_train, X_test, y_train, y_test = plays_train_test_split(plays_df_clean)\n",
    "\n",
    "# start_time = time.time()\n",
    "# model = XGBRegressor(learning_rate=1, n_estimators=200)\n",
    "# model.fit(X = X_train, y = y_train)\n",
    "# print(\"training time: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Confusion matrix: \\n\" + str(confusion_matrix(y_test, y_pred)))\n",
    "# print(\"F1 score: \" + str(round(f1_score(y_test, y_pred, average='weighted'), 3)))\n",
    "# print(\"Accuracy score: \" + str(round(accuracy_score(y_test, y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"MSE: \\n\" + str(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins = [float('-inf'), -2, 0, 1, 2.5, 5, 10, float('inf')]\n",
    "# y_pred_binned = pd.cut(y_pred, bins = bins, labels = range(len(bins) - 1))\n",
    "# y_test_binned = pd.cut(y_test, bins = bins, labels = range(len(bins) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Confusion matrix: \\n\" + str(confusion_matrix(y_test_binned, y_pred_binned)))\n",
    "# print(\"F1 score: \" + str(round(f1_score(y_test_binned, y_pred_binned, average='weighted'), 3)))\n",
    "# print(\"Accuracy score: \" + str(round(accuracy_score(y_test_binned, y_pred_binned), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
